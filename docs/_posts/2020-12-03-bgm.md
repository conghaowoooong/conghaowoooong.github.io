---
layout: post
title:  "BGM: Building a Dymaic Guidance Map without Visual Images for Trajectory Prediction"
date:   2020-12-03 20:36:32 +0800
categories: publications
---

<!--
 * @Author: ConghaoWong
 * @Date: 2019-12-20 09:37:18
 * @LastEditors: Conghao Wong
 * @LastEditTime: 2020-12-04 21:39:33
 * @Description: file contentz
 -->


# BGM Model
<div align='center'>
    <img src="https://conghaowoooong.github.io/figures/bgm.png">
    Beihao Xia, Conghao Wong
</div>

## Links
[ðŸ”— ArXiv Paper](https://arxiv.org/abs/2010.03897)

[ðŸ”— GitHub](https://github.com/conghaowoooong/BGM_prediction)

[ðŸ”— Video Introduction]({% post_url 2020-12-03-bgm_video %})

## Demo
Real-time prediction demo of BGM model on UCY-zara1 dataset.
See the high-resolution video [ðŸ”—here]({% post_url 2020-12-04-bgm_demo %}).
<div align='center'>
    <img src="https://conghaowoooong.github.io/figures/short_ver.gif">
</div>

## Abstract

Visual images usually contain the informative context of the environment, thereby helping to predict agents' behaviors.
However, they hardly impose the dynamic effects on agents' actual behaviors due to the respectively fixed semantics.
To solve this problem, we propose a deterministic model named BGM to construct a guidance map to represent the dynamic semantics, which circumvents to use visual images for each agent to reflect the difference of activities in different periods.
We first record all agents' activities in the scene within a period close to the current to construct a guidance map and then feed it to a Context CNN to obtain their context features.
We adopt a Historical Trajectory Encoder to extract the trajectory features and then combine them with the context feature as the input of the social energy based trajectory decoder, thus obtaining the prediction that meets the social rules.
Experiments demonstrate that BGM achieves state-of-the-art prediction accuracy on the two widely used ETH and UCY datasets and handles more complex scenarios.

## Overview

<div align='center'><img src="https://conghaowoooong.github.io/figures/overview.png"></div>

The BGM contains three main components:

- A Historical Trajectory Encoder
- A Dynamic Context Encoder
- A Social Energy based Trajectory Decoder, which includes the Trajectory Decoder and the Social Module

## Visualized results

<div align='center'>
<table>
    <thead>
        <tr>
            <th>conditions</th>
            <th>SR-LSTM (CVPR2019)</th>
            <th>Social GAN-P (K=20) (CVPR2018)</th>
            <th>Social STGCNN (K=20) (CVPR2020)</th>
            <th>BGM</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <th>Meet (3:28 in UCY-zara1)</th>
            <th><img src="https://conghaowoooong.github.io/figures/zara1_351_sr.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/zara1_351_sgan.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/zara1_351_stgcnn.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/zara1_351_bgm.jpg"></th>
        </tr>
        <tr>
            <th>Follow (3:28 in UCY-zara1)</th>
            <th><img src="https://conghaowoooong.github.io/figures/zara1_359_sr.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/zara1_359_sgan.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/zara1_359_stgcnn.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/zara1_359_bgm.jpg"></th>
        </tr>
        <tr>
            <th>Meet (6:47 in ETH-hotel)</th>
            <th><img src="https://conghaowoooong.github.io/figures/hotel_159_sr.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/hotel_159_sgan.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/hotel_159_stgcnn.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/hotel_159_bgm.jpg"></th>
        </tr>
        <tr>
            <th>Follow (10:53 in ETH-hotel)</th>
            <th><img src="https://conghaowoooong.github.io/figures/hotel_284_sr.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/hotel_284_sgan.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/hotel_284_stgcnn.jpg"></th>
            <th><img src="https://conghaowoooong.github.io/figures/hotel_284_bgm.jpg"></th>
        </tr>
    </tbody>
</table>
<table>
    <tbody>
        <tr>
            <th><img src="https://conghaowoooong.github.io/figures/obs.png"></th>
            <th>observations</th>
            <th><img src="https://conghaowoooong.github.io/figures/gt.png"></th>
            <th>ground truths</th>
            <th><img src="https://conghaowoooong.github.io/figures/pred.png"></th>
            <th>predictions</th>
        </tr>
    </tbody>
</table>
</div>


## Instructions
These posts could help you to build or evaluate you own model.
- [ðŸ”— Usage of Code]({% post_url 2020-12-04-bgm_code %})
- [ðŸ”— Evaluation on ETH-UCY by Our Already Trained Weights]({% post_url 2020-12-04-bgm_weights %})

## Citation
If you find this work useful, welcome to cite us by:

```
@article{wong2020bgm,
  title={BGM: Building a Dynamic Guidance Map without Visual Images for Trajectory Prediction},
  author={Xia, Beihao and Wong, Conghao and Li, Heng and Chen, Shiming and Peng, Qinmu and You, Xinge and others},
  journal={arXiv preprint arXiv:2010.03897},
  year={2020}
}
```