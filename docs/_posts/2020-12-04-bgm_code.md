---
layout: post
title:  "BGM: Usage of Codes"
date:   2020-12-04 21:08:01 +0800
categories: publications
---

<!--
 * @Author: Conghao Wong
 * @Date: 2020-12-04 19:03:16
 * @LastEditors: Conghao Wong
 * @LastEditTime: 2020-12-04 21:19:01
 * @Description: Usage of already trained models
-->

This note is about how to use BGM's code to train or test your models.
For more BGM's details, please see our [ðŸ”—homepage]({% post_url 2020-12-03-bgm %}).

## Requirements
We developed all codes with Python 3 on NVIDIA TITAN X (Pascal) GPUs.
Detailed packages used can be seen in `./requirements.txt`.
You can install all of these packages by this command:

```bash
pip install -r ./requirements.txt
```

---

## Already Trained Models
Please see another post[ðŸ”—BGM: Evaluation on ETH-UCY by Our Already Trained Weights]({% post_url 2020-12-04-bgm_weights %}) to download our already trained models and evaluate them on the ETH-UCY dataset.

---

## Train New Models on ETH-UCY Datasets
The original ETH-UCY datasets (contains ETH-eth, ETH-hotel, UCY-zara1,2,3 and UCY-univ1,3,examples) are included in `./data/eth` and `./data/ucy` with the true position in `.csv` format.
To train new models on these datasets, please use the commands above as an example:

```bash
python main.py \
    --test_set 0 \
    --epochs 500 \
    --batch_size 5000 \
    --lr 0.001 \
    --model_name example_model
```

More options can be seen below in the section `Options`.
Your new model will be saved at `./logs/YOUR_TRAINING_TIME_YOUR_MODEL_NAME/`.
(For example `./logs/20200909-120030example_model`)

---

## Train New Models on Your Datasets
If you want to train BGM on your dataset, please make your data into several `.csv` files.
Note that the data matrix's size should be `4 * Number_of_Trajectory_Positions`, and each row should as follows:

- The first row contains all the frame numbers;
- The second row contains all the pedestrian IDs;
- The third row contains all the y-coordinates;
- The fourth row contains all the x-coordinates.

Then rename these `.csv` files as `true_pos_.csv`.
After that, you should add your dataset's infomation to `./datasetManager.py` in `__init__()` of `class Manager`.
For example if you have you dataset `a, b, c, d` and their `.csv` file's save folder `./a, ./b, ./c, ./d`, you can add them by:

```python
...
class Manager():
    def __init__(self):
        self.datasets = dict()
        for dataset_name, dataset_path in zip(['a', 'b', 'c', 'd'], ['./a', './b', './c', './d'])
            self.datasets[dataset_name] = Dataset(
                # name of your dataset
                dataset     =   dataset_name,
                # save path of your dataset      
                dataset_dir =   dataset_path,
                # x-y order of your csv file        
                order       =   [1, 0],
                # leave the below four items as `[], '', [], 1` if your dataset have no videos
                # paras are [sample_rate, frame_rate] of your video 
                paras       =   [1, 30],
                # video path of your dataset
                video_path  =   dataset_path + '/video.mp4',
                # transform weights [wx, bx, wy, by] from csv file to real videos  
                weights     =   [1.0, 0.0, 1.0, 0.0],
                # scale when draw visualized results
                # visualized_shape = original_video_shape/scale
                scale       =   2,
            )

        self.set_list_YOUR_DATASET = ['a', 'b', 'c', 'd']
    ...
```

Then, please change the below parts in `./PrepareTrainData.py` in `get_train_and_test_agents()` of `class DataManager` to define your test set and train set.

```python
...
class DataManager():
    ...
    def get_train_and_test_agents(self):
        ...
        if self.args.dataset == 'YOUR_DATASET_NAME':
            self.dataset_list = self.dataset_info.set_list_YOUR_DATASET
            train_list = [i for i in self.dataset_list if not i == self.args.test_set]
            test_list = [self.args.test_set]
    
    ...
```

Commands used to train your model are the same as them used to train on the ETH-UCY dataset.
When training your model on you dataset named `'my_dataset'`, and want to test on set `a`, you can use the below commands:
```bash
python main.py --dataset my_dataset --test_set a
```

---

## Evaluate Models
After the training process, these files will be saved in you log dir:
- a model weight `YOUR_MODEL_NAME.h5` file;
- a training args `YOUR_MODEL_NAMEargs.npy` file;
- test data `YOUR_MODEL_NAMEtest.npy`.
  
### Prepare model and data
If you want to test your model on another test dataset, or your `YOUR_MODEL_NAMEtest.npy` is lost, you can use the below command to create this `test.npy` file.
For example if you want to test dataset `'a'` in `'my_dataset'` (mentioned in above *Train New Models on Your Datasets* section), please use:

```bash
python PrepareTrainData.py --dataset my_dataset --test_set a
```

Then a new `test.npy` will be created at the base `BGM_prediction` dir.
Then please rename it into `YOUR_MODEL_NAMEtest.npy` manually, and move it into your model dir.

### Run the evaluation
You can run the evaluation of your model by the following commands:

```bash
python main.py \
    --load YOUR_MODEL_DIR/YOUR_MODEL_NAME \
    --draw_results 1
```

### Visualized Results
You can draw the visualized results on the video to see the model's performance if your test dataset have videos.
Set the arg `--draw_results` to `1` to enable saving.
**Note** that make sure you have put the videos of your test dataset correctly, and you have already config your datasets' videos' path in `./datasetManager.py`.
See above *Train New Models on Your Datasets* section for details.
Default saving path of these visualized results is `./YOUR_MODEL_DIR/VisualTrajs/`.

---

## Model Options
You can custom these options both on model components and training or evaluation.
Note that all items that need a bool type of inputs should take integer `0` and `1` instead of `False` and `True`.
To passing your customed args, please use the below format:
```bash
python main.py --ARGS_YOU_WANT_TO_CHANGE ARGS_VALUE
```

**Environment options**:

- `--gpu`:
(Optional) Choose which GPU the model training or evaluation on.
This parameter should be a positive integer.
If you have no GPUs or want to run the code on your CPU, please set it to `-1`.


**Model options**:

- `--obs_frames`:
Length of historical trajectories.
Default is `8`.
- `--pred_frames`:
Length of predictions.
Default is `12`.
- `--dropout`:
The rate of dropout.
Default is `0.5`.
- `--calculate_social`:
Controls whether the model predicts the agent's neighbors' trajectories.
Default is `False`.
It will be set to `True` automatically when `--sr_enable` has been set to `True`.
- `--sr_enable`:
Controls whether to enable the Social Module.
Default is `False`.

**Social module options**:

- `--grid_shape_x`:
Grid width of the social energy map.
Default is `700`.
- `--grid_shape_y`:
Grid height of the social energy map.
Default is `700`.
- `--grid_length`:
Resolution of each grid in meters.
Default is `0.1`.
- `--avoid_size`:
Ideal distance to avoid collision with other people in grid size.
Default is `15`.
- `--interest_size`:
Ideal distance to be attracted by the agent's original intention in grid size.
Default is `20`.
- `--max_refine`:
The maximum limit that the social module could change in meters.
Default is `0.8`.

**Guidance map options**:

Args with * represents those args have not been transferred to `main.py`.
Please change them manually when needed.

- `--window_size_map`*:
Resolution of the guidance map that shows the number of grids used to represent the reality 1 meter.
Default is `4`.
(Current in `./GridRefine.py`)
- `--local_half_size`*:
The half side length of the local guidance maps in grid size.
Default is `16`.
(Current in `./PrepareTrainData.py`)

**Training and evaluation options**:

- `--test_set`:
The test set of the current model.
In ETH-UCY datasets, eth=`0`, hotel=`1`, zara1=`2`, zara2=`3` and univ=`4`.
When using your datasets, please refer to the above sections.
Default is `2`.
You can also pass this arg by the dataset's name.
- `--load`:
The model path to be evaluated.
Set `'null'` if you want to train new models.
Default is `'null'`.
- `--train_percent`:
Controls the percent of the training set used to train the model.
If you have five datasets, you could use the command `--train_percent 0.1 0.3 0.5 0.7 0.9` to controls the percent each dataset used.
You can also give it one float number like `--train_percent 0.1` to set all training sets the same percent.
Default is `1.0`.
- `--reverse`:
Set whether to use the reversation to strengthen training data.
Default is `False`.
- `--add_noise`:
Set whether to use the additional noise data to strengthen the training set.
Set `False` if you do not need this strengthen method, and select a positive integer `n` to add `n` times of noise data to the original training set.
Default is `False`.
- `--rotate`:
Set whether to use the additional rotation data to strengthen the training set.
Set `False` if you do not need this strengthen method, and set `n` means that the model will divide the 360 degrees into `n` parts, and add all other new degree's rotation trajectories except 0 degrees to the original training set.
Default is `False`.
- `--test`:
Set `True` to enable test during training.
It will be set to `True` automatically when `--load` is not `'null'`.
Default is `True`.
- `--start_test_percent`:
Controls when to start tests during training.
The parameter should be a 0~1 float value.
Default is `0.0`.
It only works when `--test` is set to `True`.
- `--test_step`:
Controls the number of epochs between two tests during training.
Default is `3`.
- `--epochs`:
The number of epochs when training.
Default is `500`.
- `--lr`:
Learning rate.
Default is `0.001`.
- `--batch_size`:
Batch size.
Default is `500`.

**Save args**:

- `--model_name`:
The name of your new model.
Default is `'model'`.
- `--save_model`:
Controls if you want to save the trained model after training.
Default is `True`.
- `--log_dir`:
The root dir where model saved.
Your model will be saved at `./logs/YOUR_TRAINING_TIME_YOUR_MODEL_NAME` when leaving it `'null'`.
Default is `'null'`.

See `./argManager.py` for more args.
